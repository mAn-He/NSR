{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6ce22d2-d581-408c-bea8-3d6ddf94e13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sflab/SFLAB/sungheon/nsr/public/engines/week16_engines_knn_no_scaling.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_sub['상품코드_칼라명'] = df_test_sub['상품코드'] + '_' + df_test_sub['칼라명2']\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "# from engines.models_transformer import *g\n",
    "from engines.models_knn import *\n",
    "from engines.week16_engines_knn_no_scaling import *\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "def none_or_int(value):\n",
    "    if value.lower() == 'none':\n",
    "        return None\n",
    "    return int(value)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training script for MultiModalMOdel')\n",
    "\n",
    "parser.add_argument('--device',type=int,default=0,help='device number for gpu accelerating (default = 0)')\n",
    "parser.add_argument('--batch_size',type=int,default=64,help='batch size (default=64)')\n",
    "parser.add_argument('--image_embeddings_dim_out', type=none_or_int, default=128, help='Add Linear Layer, Output dimension for image embeddings (default=128, set \"None\" for no linear embedding proj)')\n",
    "parser.add_argument('--text_embeddings_dim_out', type=none_or_int, default=128, help='Add Linear Layer, Output dimension for text embeddings (default=128, set \"None\" for no linear embedding proj))')\n",
    "parser.add_argument('--other_features_dim_out', type=none_or_int, default=None, help='Add Linear Layer, Output dimension for other features embeddings (default=128, set \"None\" for no linear embedding proj))')\n",
    "parser.add_argument('--header_mode',type=str,default='FFN',help='FFN, Dense, Transformer, FFN_Transformer, Dense_Transformer')\n",
    "parser.add_argument('--header_hidden_dims', type=int, nargs='+', default=[128], help='Add Linear Layer, Hidden dimensions for the header (default=128)')\n",
    "parser.add_argument('--dir_path', type=str, default='/home/sflab/SFLAB/sungheon/nsr/public/results_folder/mse_save_model_unscal_2/full_3_ffn', help='Directory path to save the model state dict (default=\"model_state_dict\")') \n",
    "parser.add_argument('--train_dataset',type=str, default='../dataset/nsr_train_할인율0_중분류.csv',help='train dataset path (default=\"../dataset/nsr_train_할인율0_중분류.csv\")')\n",
    "parser.add_argument('--test_dataset',type=str, default='../dataset/nsr_test_할인율0_중분류.csv',help='test dataset path (default=\"../dataset/nsr_test_할인율0_중분류.csv\")')\n",
    "parser.add_argument('--random_seed',type=int, default=42,help='random seed (default=42)')\n",
    "parser.add_argument('--image_size',type=int,default=256,help='image size for transforming (default=256)')\n",
    "parser.add_argument('--learning_rate',type=float,default=0.01,help='learning rate (default=0.0000001)')\n",
    "parser.add_argument('--num_epochs',type=int,default=15,help='Num epochs for training (default=50)')\n",
    "parser.add_argument('--loss_ratio',type=float,default=0,help='Scaled/Unscaled Loss ratio, default=1(totally weight on target Scaled loss (default=1)')\n",
    "parser.add_argument('--image_normalizing',type=bool,default=False,help='Decide wheter use image nomralizing or not(default=False)')\n",
    "parser.add_argument('--nhead',type=int,default=4,help='transformer nhead')\n",
    "parser.add_argument('--num_encoder_layers',type=int,default=6,help='transformer num encoder layers')\n",
    "parser.add_argument('--num_decoder_layers',type=int,default=6,help='transformer num decoder layers')\n",
    "parser.add_argument('--model_path',type=str,default='engines/models_knn2.py',help='model.py path')\n",
    "parser.add_argument('--activation_func',type=str,default='gelu',help='gelu or relu')\n",
    "parser.add_argument('--knn_metric',type=str,default='euclidean',help='euclidean or cosine')\n",
    "parser.add_argument('--k_neighbors',type=int,default=3,help='neighbors')\n",
    "parser.add_argument('--knn_embedding_metadata_concat',type=bool,default=False,help='Decide whether concatenate knn_embedding and metadata')\n",
    "\n",
    "if '--help' in sys.argv or '-h' in sys.argv:\n",
    "    parser.print_help()\n",
    "    sys.exit()\n",
    "    \n",
    "if 'ipykernel' in sys.modules:\n",
    "    # Jupyter 노트북에서 실행 중인 경우\n",
    "    args = parser.parse_args(args=[])\n",
    "else:\n",
    "    # 일반 스크립트로 실행 중인 경우\n",
    "    args = parser.parse_args()\n",
    "\n",
    "set_random_seed(args.random_seed)\n",
    "\n",
    "os.makedirs(args.dir_path, exist_ok=True)\n",
    "config_path = os.path.join(args.dir_path, 'config.json')\n",
    "with open(config_path, 'w',encoding='utf-8') as f:\n",
    "    json.dump(vars(args), f, indent=4,ensure_ascii=False)\n",
    "\n",
    "df_train = pd.read_csv(args.train_dataset,index_col=0)\n",
    "df_test = pd.read_csv(args.test_dataset,index_col=0)\n",
    "df_train_true = df_train.copy()\n",
    "df_test_true = df_test.copy()\n",
    "\n",
    "hi_train = pd.DataFrame()\n",
    "hi_train['상품코드'] = df_train['상품코드']\n",
    "hi_train['칼라'] = df_train['칼라명2']\n",
    "hi_train['판매수량_true'] = df_train['판매수량']\n",
    "\n",
    "\n",
    "hi_test = pd.DataFrame()\n",
    "hi_test['상품코드'] = df_test['상품코드']\n",
    "hi_test['칼라'] = df_test['칼라명2']\n",
    "hi_test['판매수량_true'] = df_test['판매수량']\n",
    "\n",
    "y_train = np.array(df_train['판매수량'].tolist())\n",
    "y_test = np.array(df_test['판매수량'].tolist())\n",
    "\n",
    "device = torch.device(f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "df_train, df_test = get_nearest_neighbors(df_train,df_test,device=device,k=args.k_neighbors,dis_metric=args.knn_metric,image_normalizing=args.image_normalizing,image_resize_shape=(args.image_size,args.image_size),image_embedding_dim=args.image_embeddings_dim_out)\n",
    "\n",
    "df_train = df_train.drop(columns=['Unnamed: 0', '판매시작연도', '판매첫날', '상품코드', '판매일자', '상품명', '상품명2', '칼라', '칼라명', '칼라명2', '현재가', '할인율(%)', '파일경로', '이미지갯수', '외관설명', '기능설명','카테고리'],errors='ignore')\n",
    "df_train = df_train.drop(df_train.filter(like='closest idx').columns,axis=1,errors='ignore')\n",
    "\n",
    "cols = df_train.columns.tolist()\n",
    "\n",
    "df_test = df_test[cols]\n",
    "hi_train = hi_train.drop_duplicates()\n",
    "hi_test = hi_test.drop_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "22aa0800-bd56-40c5-bebc-567a8c2c40ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_neighbors_mean(df,neighbor_col,target_col,normalize_method):\n",
    "    \"\"\"\n",
    "    normalize_method : 'minmax' : MinMaxScaler, 'standard' : StandardScaler\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "    normalizing = {'minmax':MinMaxScaler(),\n",
    "                   'standard':StandardScaler()}\n",
    "    scaler = normalizing[normalize_method]\n",
    "    df2 = df.copy()\n",
    "    df2['neighbor mean'] = df2[neighbor_col].apply(lambda x: np.mean(x))\n",
    "    df2['distance'] = df2[target_col] - df2['neighbor mean']\n",
    "\n",
    "    mean = df2['distance'].mean()\n",
    "    std = df2['distance'].std()\n",
    "\n",
    "    df2['normalized distance'] = scaler.fit_transform(df2['distance'].values.reshape(-1,1))\n",
    "    return df2, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb6de2f4-5e8b-4667-b064-35fa4ac0d16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train2, scaler = get_k_neighbors_mean(df_train,f\"{args.k_neighbors} closest 판매수량\",'판매수량','minmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "06a47a51-8aad-4b60-93af-1e0dd490077d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>이미지파일</th>\n",
       "      <th>Color_BLACK</th>\n",
       "      <th>Color_BLACK DEEPESTRED</th>\n",
       "      <th>Color_BLACKBLACK</th>\n",
       "      <th>Color_BLACKCHARCOAL</th>\n",
       "      <th>Color_BLACKDGREY</th>\n",
       "      <th>Color_BLACKRED</th>\n",
       "      <th>Color_DEEPGR</th>\n",
       "      <th>Color_KHAKI</th>\n",
       "      <th>Color_MAROON</th>\n",
       "      <th>...</th>\n",
       "      <th>외관설명_벡터_766</th>\n",
       "      <th>외관설명_벡터_767</th>\n",
       "      <th>중분류_(통)긴바지</th>\n",
       "      <th>중분류_5/7부</th>\n",
       "      <th>중분류_9부</th>\n",
       "      <th>판매수량</th>\n",
       "      <th>3 closest 판매수량</th>\n",
       "      <th>neighbor mean</th>\n",
       "      <th>distance</th>\n",
       "      <th>normalized distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>상품코드_칼라명</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NSA1I01_BLACK</th>\n",
       "      <td>['../dataset/images/EVOKE TIGHTS MEN/BLACK/BLA...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.146764</td>\n",
       "      <td>0.091771</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>[33, 190, 219]</td>\n",
       "      <td>147.333333</td>\n",
       "      <td>-79.333333</td>\n",
       "      <td>0.240531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 1560 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           이미지파일  Color_BLACK  \\\n",
       "상품코드_칼라명                                                                        \n",
       "NSA1I01_BLACK  ['../dataset/images/EVOKE TIGHTS MEN/BLACK/BLA...         True   \n",
       "\n",
       "               Color_BLACK DEEPESTRED  Color_BLACKBLACK  Color_BLACKCHARCOAL  \\\n",
       "상품코드_칼라명                                                                       \n",
       "NSA1I01_BLACK                   False             False                False   \n",
       "\n",
       "               Color_BLACKDGREY  Color_BLACKRED  Color_DEEPGR  Color_KHAKI  \\\n",
       "상품코드_칼라명                                                                     \n",
       "NSA1I01_BLACK             False           False         False        False   \n",
       "\n",
       "               Color_MAROON  ...  외관설명_벡터_766  외관설명_벡터_767  중분류_(통)긴바지  \\\n",
       "상품코드_칼라명                     ...                                         \n",
       "NSA1I01_BLACK         False  ...    -0.146764     0.091771           0   \n",
       "\n",
       "               중분류_5/7부  중분류_9부  판매수량  3 closest 판매수량  neighbor mean  \\\n",
       "상품코드_칼라명                                                               \n",
       "NSA1I01_BLACK         0       1    68  [33, 190, 219]     147.333333   \n",
       "\n",
       "                distance  normalized distance  \n",
       "상품코드_칼라명                                       \n",
       "NSA1I01_BLACK -79.333333             0.240531  \n",
       "\n",
       "[1 rows x 1560 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc3f9be-b956-496d-9021-afb78d4f02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from engines.week16_engines_knn_no_box import *\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox,boxcox1p,inv_boxcox1p\n",
    "\n",
    "class MultiModalModel(nn.Module):\n",
    "    def __init__(self, num_images_per_data=2,\n",
    "                 header_mode:str='ffn',\n",
    "                 image_embeddings_dim_out=None, text_embedding_dim_in=None, \n",
    "                 text_embedding_dim_out=None, other_features_dim_in=None, \n",
    "                 other_features_dim_out=None, header_hidden_dims:list=None,\n",
    "                 knn_embedding_dim:int=3,\n",
    "                 knn_embedding_metadata_concat=False,\n",
    "                 nhead:int=8, num_encoder_layers:int=6,num_decoder_layers:int=6,activation:str='gelu'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        header_mode : {'ffn', 'dense', 'transformer','ffn_transformer','dense_transformer'} 이 중에서 선택해주세요.\n",
    "        \n",
    "        image_embeddings_dim_out : 원하는 이미지 emedding dimension (맘대로 가능)\n",
    "        text_embedding_dim_in : dataframe의 텍스트 칼럼 dimension (데이터 프레임에 있는 텍스트 임베딩 칼럼 갯수.. 고정값)\n",
    "        text_embedding_dim_out : 선형 변환을 통한 텍스트 embedding dim (맘대로 가능)\n",
    "        other_features_dim_in : dataframe의 텍스트 embedding 칼럼 및 이미지, target 칼럼 제외, 나머지 feature 칼럼들 dimension (데이터 프레임에 있는 텍스트 제외한 칼럼들 갯수.. 고정값)\n",
    "        other_features_dim_out : 선형 변환을 통한 output feature dim (맘대로 가능)\n",
    "        header_hidden_dims : 각 모델의 ffn이나, dense 블록의 hidden dim\n",
    "        nhead : transformer 계열 모델에서 num head,\n",
    "        num_encoder_layers : transformer의 encoder layer 갯수\n",
    "        num_decoder_layers : transformer의 decoder layer 갯수\n",
    "        \"\"\"\n",
    "        self.knn_concat = knn_embedding_metadata_concat\n",
    "        # Image\n",
    "        self.header_mode = header_mode\n",
    "      \n",
    "        self.image_model_weights = torchvision.models.ResNet152_Weights.DEFAULT\n",
    "        self.image_model = torchvision.models.resnet152(weights=self.image_model_weights)\n",
    "        \n",
    "        if image_embeddings_dim_out is not None:\n",
    "            self.image_model.fc = nn.Linear(in_features=self.image_model.fc.in_features, out_features=image_embeddings_dim_out)\n",
    "            self.image_output_dim = image_embeddings_dim_out\n",
    "        else:\n",
    "            self.image_output_dim = self.image_model.fc.in_features\n",
    "            self.image_model.fc = nn.Identity()\n",
    "\n",
    "        # Text\n",
    "        if text_embedding_dim_out is not None:\n",
    "            self.text_fc = nn.Linear(in_features=text_embedding_dim_in, out_features=text_embedding_dim_out)\n",
    "            self.text_embedding_dim = text_embedding_dim_out\n",
    "        else:\n",
    "            self.text_fc = nn.Identity()\n",
    "            self.text_embedding_dim = text_embedding_dim_in\n",
    "\n",
    "        # 나머지 feature\n",
    "        if other_features_dim_out is not None:\n",
    "            if self.knn_concat is True:\n",
    "                self.rest_feature_fc = nn.Linear(in_features=other_features_dim_in + knn_embedding_dim, out_features=other_features_dim_out)\n",
    "                self.other_features_dim = other_features_dim_out\n",
    "\n",
    "            else:\n",
    "                self.rest_feature_fc = nn.Linear(in_features=other_features_dim_in, out_features=other_features_dim_out)\n",
    "                self.other_features_dim = other_features_dim_out\n",
    "        else:\n",
    "            if self.knn_concat is True:\n",
    "\n",
    "                self.rest_feature_fc = nn.Identity()\n",
    "                self.other_features_dim = other_features_dim_in + knn_embedding_dim\n",
    "\n",
    "            else:\n",
    "                self.rest_feature_fc = nn.Identity()\n",
    "                self.other_features_dim = other_features_dim_in\n",
    "\n",
    "                \n",
    "\n",
    "        print('image_output_dim :',self.image_output_dim)\n",
    "        print('text_embedding_dim :',self.text_embedding_dim)\n",
    "        print('other_features_dim :',self.other_features_dim)\n",
    "        # 막단 layer\n",
    "\n",
    "        if self.knn_concat is True:\n",
    "            self.input_dim = self.image_output_dim * num_images_per_data + self.text_embedding_dim + self.other_features_dim\n",
    "\n",
    "        else:\n",
    "            self.input_dim = self.image_output_dim * num_images_per_data + self.text_embedding_dim + self.other_features_dim + knn_embedding_dim\n",
    "        \n",
    "        self.head_mlp = MLPRegression(self.input_dim, hidden_dims=header_hidden_dims, dropout_prob=0.2,activation=activation)\n",
    "        \n",
    "        \n",
    "        if activation.lower() == 'relu':\n",
    "            self.activ = nn.ReLU()\n",
    "        elif activation.lower() == 'gelu':\n",
    "            self.activ = nn.GELU()\n",
    "            \n",
    "        self.dense_block = Dense_block(self.input_dim,hidden_dims=header_hidden_dims,activation=activation)\n",
    "        \n",
    "        print('concated embed dim :',self.input_dim)\n",
    "\n",
    "        if self.header_mode == 'transformer':\n",
    "            if self.input_dim % nhead != 0:\n",
    "        \n",
    "                raise ValueError(f\"concated embed dim (= {self.image_output_dim + self.text_embedding_dim + self.other_features_dim}) 은 nhead로 나누어 떨어져야만 합니다\")\n",
    "            \n",
    "            else:\n",
    "                self.transformer = nn.Transformer(d_model = self.input_dim,\n",
    "                                          nhead = nhead,\n",
    "                                          num_encoder_layers = num_encoder_layers,\n",
    "                                          num_decoder_layers = num_decoder_layers,\n",
    "                                          batch_first=True)\n",
    "                self.fc = nn.Linear(self.input_dim,1)\n",
    "        # if self.header_mode.lower() == 'ffn':\n",
    "        #     self.header = self.head_mlp\n",
    "\n",
    "        # elif self.header_mode.lower() == 'dense':\n",
    "        #     self.header = self.dense_block\n",
    "\n",
    "        # elif self.header_mode.lower() == 'transformer':\n",
    "        \n",
    "            \n",
    "\n",
    "        \n",
    "\n",
    "        if self.header_mode.lower() == 'ffn_transformer':\n",
    "            self.bottle = nn.Sequential(*list(*self.head_mlp.children())[:-1])\n",
    "            self.bottle_out_dim = list(self.bottle)[-4].out_features\n",
    "            if self.bottle_out_dim % nhead != 0:\n",
    "        \n",
    "                raise ValueError(f\"Bottle Neck Output Dim (={self.bottle_out_dim}) 은 nhead로 나누어 떨어져야만 합니다\")\n",
    "            \n",
    "            else:\n",
    "                self.transformer = nn.Transformer(d_model = self.bottle_out_dim,\n",
    "                                              nhead = nhead,\n",
    "                                              num_encoder_layers = num_encoder_layers,\n",
    "                                              num_decoder_layers = num_decoder_layers,\n",
    "                                              batch_first=True)\n",
    "                self.fc = nn.Linear(self.bottle_out_dim,1)\n",
    "                \n",
    "        elif self.header_mode.lower() == 'dense_transformer':\n",
    "            self.bottle = nn.Sequential(*list(*self.dense_block.children())[:-1])\n",
    "            self.bottle_out_dim = list(self.bottle)[-1].out_features\n",
    "            \n",
    "            if self.bottle_out_dim % nhead != 0:\n",
    "        \n",
    "                raise ValueError(f\"Bottle Neck Output Dim (={self.bottle_out_dim}) 은 nhead로 나누어 떨어져야만 합니다\")\n",
    "            \n",
    "            else:\n",
    "                self.transformer = nn.Transformer(d_model = self.bottle_out_dim,\n",
    "                                              nhead = nhead,\n",
    "                                              num_encoder_layers = num_encoder_layers,\n",
    "                                              num_decoder_layers = num_decoder_layers,\n",
    "                                              batch_first=True) \n",
    "                self.fc = nn.Linear(self.bottle_out_dim,1)\n",
    "                \n",
    "    def forward(self, images, text, other_features, knn_embedding):\n",
    "        image_embeddings = [self.activ(self.image_model(image)) for image in images]\n",
    "        image_embeddings = torch.stack(image_embeddings,dim=0)\n",
    "        \n",
    "        image_embeddings2 = torch.flatten(image_embeddings, start_dim=1)\n",
    "\n",
    "        text_embeddings = self.text_fc(text)\n",
    "        text_embeddings = self.activ(text_embeddings)\n",
    "\n",
    "        if self.knn_concat is True:\n",
    "\n",
    "            other_embeddings = torch.cat((other_features,knn_embedding),dim=1)\n",
    "            other_embeddings = self.rest_feature_fc(other_embeddings)\n",
    "            other_embeddings = self.activ(other_embeddings)\n",
    "            combined_embeddings = torch.cat((image_embeddings2, text_embeddings, other_embeddings), dim=1)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            other_embeddings = self.rest_feature_fc(other_features)\n",
    "            other_embeddings = self.activ(other_embeddings)\n",
    "            combined_embeddings = torch.cat((image_embeddings2, text_embeddings, other_embeddings,knn_embedding), dim=1)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "        y =self.activ(combined_embeddings)\n",
    "\n",
    "        if self.header_mode.lower() == 'ffn':\n",
    "            y = self.head_mlp(y)\n",
    "\n",
    "        elif self.header_mode.lower() == 'dense':\n",
    "            y = self.dense_block(y)\n",
    "\n",
    "        elif self.header_mode.lower() == 'transformer':\n",
    "            \n",
    "            y = self.transformer(y,y)\n",
    "            y = self.activ(y)\n",
    "            y = self.fc(y)\n",
    "\n",
    "        else:\n",
    "\n",
    "            y = self.bottle(y)\n",
    "            y = self.transformer(y,y)\n",
    "            y = self.activ(y)\n",
    "            y = self.fc(y)\n",
    "           \n",
    "\n",
    "        return nn.Sigmoid(y)\n",
    "\n",
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, dropout_prob=0.2,activation='gelu'):\n",
    "        super(MLPRegression, self).__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "\n",
    "            \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))  # Batch Normalization\n",
    "            if activation.lower() == 'gelu':\n",
    "                layers.append(nn.GELU())\n",
    "            elif activation.lower() == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_prob))  # Dropout\n",
    "            in_features = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(in_features, 1))  # Output layer for regression\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Dense_block(nn.Module):\n",
    "    def __init__(self, input_dim,hidden_dims,activation='gelu'):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_features = input_dim\n",
    "        \n",
    "    \n",
    "            \n",
    "        if hidden_dims:\n",
    "            for hidden_dim in hidden_dims:\n",
    "                layers.append(nn.Linear(in_features,hidden_dim))\n",
    "                if activation.lower() == 'gelu':\n",
    "                    layers.append(nn.GELU())\n",
    "                elif activation.lower() == 'relu':\n",
    "                    layers.append(nn.ReLU())\n",
    "\n",
    "                in_features = hidden_dim\n",
    "\n",
    "        elif hidden_dims == 0 or \"None\":\n",
    "            pass\n",
    "\n",
    "        layers.append(nn.Linear(in_features,1))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8e3695b1-7fa7-48f0-a481-86cabde04c3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1763582/4185577840.py:1: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  scaler.inverse_transform(torch.tensor(df_train2['distance'],dtype=torch.float).reshape(-1,1))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -54003.89061228],\n",
       "       [  82433.        ],\n",
       "       [  56229.89061228],\n",
       "       [  12859.22265307],\n",
       "       [ -85628.33333333],\n",
       "       [  -8826.11132654],\n",
       "       [ -35255.11197281],\n",
       "       [  -8600.2220068 ],\n",
       "       [ -49937.89061228],\n",
       "       [ -40902.33333333],\n",
       "       [   9696.7779932 ],\n",
       "       [ -55811.        ],\n",
       "       [  34544.55469386],\n",
       "       [  26186.66666667],\n",
       "       [  29800.88802719],\n",
       "       [   7212.        ],\n",
       "       [  -3178.8889966 ],\n",
       "       [-121770.55210876],\n",
       "       [  -5211.8889966 ],\n",
       "       [ -26219.55469386],\n",
       "       [   2468.33333333],\n",
       "       [  -3178.8889966 ],\n",
       "       [  -7019.        ],\n",
       "       [ -14699.22265307],\n",
       "       [ -18539.33333333],\n",
       "       [-150684.33333333],\n",
       "       [-160623.44789124],\n",
       "       [  -7019.        ],\n",
       "       [  68879.66666667],\n",
       "       [ -35932.77863948],\n",
       "       [  91242.66666667],\n",
       "       [ -26897.22136052],\n",
       "       [ -43161.22136052],\n",
       "       [  51486.22394562],\n",
       "       [ -51519.10938772],\n",
       "       [  -9955.55534013],\n",
       "       [  15344.        ],\n",
       "       [ -64168.89061228],\n",
       "       [  -7922.55534013],\n",
       "       [  49905.        ],\n",
       "       [   2920.1110034 ],\n",
       "       [-164463.55210876],\n",
       "       [-105506.55210876],\n",
       "       [ -77044.55727895],\n",
       "       [ -24412.44530614],\n",
       "       [-103699.44789124],\n",
       "       [  18506.4440136 ],\n",
       "       [ -81788.22394562],\n",
       "       [ -16054.5559864 ],\n",
       "       [  -1597.66666667],\n",
       "       [  -8600.2220068 ],\n",
       "       [ -52874.44272105],\n",
       "       [ -59651.10938772],\n",
       "       [ -30059.66666667],\n",
       "       [ -33222.11197281],\n",
       "       [  -6793.11132654],\n",
       "       [  14440.4440136 ],\n",
       "       [ 175499.2291158 ],\n",
       "       [ -41580.        ],\n",
       "       [  23024.22136052],\n",
       "       [  -2275.33333333],\n",
       "       [ -89242.55210876],\n",
       "       [ -30963.22136052],\n",
       "       [ -53326.22394562],\n",
       "       [ 114283.33333333],\n",
       "       [ -27800.77863948],\n",
       "       [-114316.21877543],\n",
       "       [ -68912.55727895],\n",
       "       [  21668.88802719],\n",
       "       [  27090.22136052],\n",
       "       [-111379.66666667],\n",
       "       [ -40676.44530614],\n",
       "       [ 103892.44789124],\n",
       "       [ -72752.66666667],\n",
       "       [ -43387.11197281],\n",
       "       [  52389.77605438],\n",
       "       [  -7922.55534013],\n",
       "       [  85821.33333333],\n",
       "       [ 124222.44789124],\n",
       "       [ -39998.77863948],\n",
       "       [  48775.55727895],\n",
       "       [ 199669.33333333],\n",
       "       [  27993.77863948],\n",
       "       [  51938.        ],\n",
       "       [  68427.89061228],\n",
       "       [   4275.44433673],\n",
       "       [ -29833.77863948],\n",
       "       [ -49260.22394562],\n",
       "       [ -31640.88802719],\n",
       "       [ 109765.55210876],\n",
       "       [ 261562.89578247],\n",
       "       [ -15376.88931974],\n",
       "       [  82207.10938772],\n",
       "       [-157686.8854421 ],\n",
       "       [  16247.5559864 ],\n",
       "       [ -38417.55469386],\n",
       "       [  22346.55469386],\n",
       "       [-124481.21877543],\n",
       "       [ 294768.56244914],\n",
       "       [ -92856.78122457],\n",
       "       [ 250946.10421753],\n",
       "       [ -14247.4440136 ],\n",
       "       [ -80658.77605438],\n",
       "       [  24153.66666667],\n",
       "       [  58714.66666667],\n",
       "       [ -59651.10938772],\n",
       "       [ -40224.66666667],\n",
       "       [  31608.        ]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scaler.inverse_transform(torch.tensor(df_train2['distance'],dtype=torch.float).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48842cbb-b0c4-4af2-baad-1a3207797fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_nearest_neighbors(df_train,df_test,device,image_embedding_csv=None,k:int=3,dis_metric:str='euclidean',image_normalizing=False,image_resize_shape:int=256,image_embedding_model=None,image_embedding_dim:int=128):\n",
    "    from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "    from PIL import Image\n",
    "\n",
    "    metric_dict = {'euclidean':euclidean_distances,\n",
    "                   'cosine':cosine_similarity}\n",
    "\n",
    "    dis_metric = metric_dict[dis_metric]\n",
    "\n",
    "    columns_to_drop = ['판매시작연도', '현재가', '이미지갯수', '파일경로', '할인율(%)', '판매첫날', '판매일자', '상품명', '상품명2', '칼라', '칼라명', '외관설명', '기능설명', '카테고리']\n",
    "    \n",
    "    df_train_sub = df_train.drop(columns=columns_to_drop,errors='ignore')\n",
    "    df_test_sub = df_test[df_train_sub.columns]\n",
    "\n",
    "    df_train_sub['상품코드_칼라명'] = df_train_sub['상품코드'] + '_' + df_train_sub['칼라명2']\n",
    "    df_test_sub['상품코드_칼라명'] = df_test_sub['상품코드'] + '_' + df_test_sub['칼라명2']\n",
    "\n",
    "    df_train_sub = df_train_sub.drop('칼라명2',axis=1)\n",
    "    df_test_sub = df_test_sub.drop('칼라명2',axis=1)\n",
    "    \n",
    "    df_train_sub = df_train_sub.drop_duplicates()\n",
    "    df_test_sub = df_test_sub.drop_duplicates()\n",
    "    \n",
    "    df_train_sub = df_train_sub.set_index('상품코드_칼라명')\n",
    "    df_test_sub = df_test_sub.set_index('상품코드_칼라명')\n",
    "    \n",
    "    df_train_image = df_train_sub[['이미지파일']]\n",
    "    df_test_image = df_test_sub[['이미지파일']]\n",
    "\n",
    "    df_train_sell = df_train_sub[['판매수량']]\n",
    "    df_test_sell = df_test_sub[['판매수량']]\n",
    "    \n",
    "    df_train_sub = df_train_sub.drop(columns='판매수량')\n",
    "    df_test_sub = df_test_sub.drop(columns='판매수량')\n",
    "    \n",
    "    df_train_index = df_train_sub.index\n",
    "    df_test_index = df_test_sub.index\n",
    "    \n",
    "    if image_embedding_model is None:\n",
    "        extractor_weight = torchvision.models.ResNet152_Weights.DEFAULT\n",
    "        extractor = torchvision.models.resnet152(weights = extractor_weight)\n",
    "\n",
    "    else:\n",
    "        extractor = image_embedding_model\n",
    "\n",
    "    if image_embedding_dim is None:\n",
    "        \n",
    "        extractor = nn.Sequential(*list(extractor.children())[:-1],\n",
    "                                  nn.Flatten())\n",
    "\n",
    "    else:\n",
    "        \n",
    "        input_dim = list(extractor.children())[-1].in_features\n",
    "        extractor = nn.Sequential(*list(extractor.children())[:-1],\n",
    "                                  nn.Flatten(),\n",
    "                                 nn.Linear(input_dim,image_embedding_dim)\n",
    "                                 )\n",
    "    assert image_normalizing in [True, False], \"image_normalizing must be either True or False\"\n",
    "    \n",
    "    if image_normalizing is False:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_resize_shape),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_resize_shape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    if image_embedding_csv:\n",
    "        \n",
    "        df_train_sub = pd.concat([df_train_sub,image_embedding_csv],axis=1)\n",
    "        df_test_sub = pd.concat([df_test_sub,image_embedding_csv],axis=1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        train_embedding_list = []\n",
    "        test_embedding_list = []\n",
    "        extractor.eval()\n",
    "        extractor.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for row in df_train_image['이미지파일'].apply(lambda x : ast.literal_eval(x)):\n",
    "                images = [Image.open(image) for image in row]\n",
    "                images = [transform(image) for image in images]\n",
    "                \n",
    "                if len(images) == 1:\n",
    "                    images.append(images[-1])\n",
    "\n",
    "\n",
    "                images = torch.stack(images)\n",
    "                images = images.to(device)\n",
    "                image_embed = extractor(images)\n",
    "                image_embed = image_embed.flatten()\n",
    "                train_embedding_list.append(image_embed.detach().cpu().numpy())\n",
    "\n",
    "            for row in df_test_image['이미지파일'].apply(lambda x : ast.literal_eval(x)):\n",
    "                images = [Image.open(image) for image in row]\n",
    "                images = [transform(image) for image in images]\n",
    "                \n",
    "                if len(images) == 1:\n",
    "                    images.append(images[0])\n",
    "\n",
    "\n",
    "                images = torch.stack(images)\n",
    "                images = images.to(device)\n",
    "                image_embed = extractor(images)\n",
    "                image_embed = image_embed.flatten()\n",
    "                test_embedding_list.append(image_embed.detach().cpu().numpy())\n",
    "                \n",
    "    train_embedding_list = np.array(train_embedding_list)\n",
    "    test_embedding_list = np.array(test_embedding_list)\n",
    "    \n",
    "    df_train_img_embed = pd.DataFrame(train_embedding_list,index=df_train_index,columns=[f'image_embed_{i}' for i in range(train_embedding_list.shape[-1])])\n",
    "    df_test_img_embed = pd.DataFrame(test_embedding_list,index=df_test_index,columns=[f'image_embed_{i}' for i in range(test_embedding_list.shape[-1])])        \n",
    "    \n",
    "    df_train_sub2 = df_train_sub.drop(['이미지파일','상품코드'],axis=1)\n",
    "    df_test_sub2 = df_test_sub.drop(['이미지파일','상품코드'],axis=1)\n",
    "    \n",
    "    df_train_embeddings = pd.concat([df_train_sub2,df_train_img_embed],axis=1)\n",
    "    df_test_embeddings = pd.concat([df_test_sub2,df_test_img_embed],axis=1)\n",
    "\n",
    "    df_embeddings = pd.concat([df_train_embeddings,df_test_embeddings],axis=0)\n",
    "    df_embeddings_index = df_embeddings.index\n",
    "    \n",
    "    metric = dis_metric(df_embeddings)\n",
    "    np.fill_diagonal(metric,np.inf)\n",
    "    df_metric = pd.DataFrame(metric,index=df_embeddings_index, columns=df_embeddings_index)\n",
    "    df_metric = df_metric[df_train_embeddings.index]\n",
    "\n",
    "    df_train_sub = pd.concat([df_train_sub,df_train_sell],axis=1)\n",
    "    df_test_sub = pd.concat([df_test_sub, df_test_sell],axis=1)\n",
    "    \n",
    "    df_metric['closest'] = df_metric.apply(lambda x: x.nsmallest(k).index.values, axis=1)\n",
    "    df_train_sub[f'{k} closest idx'] = df_metric.loc[df_train_sub.index, 'closest']\n",
    "    df_test_sub[f'{k} closest idx'] = df_metric.loc[df_test_sub.index,'closest']\n",
    "\n",
    "    df_train_sub[f'{k} closest 판매수량'] = df_train_sub[f'{k} closest idx'].apply(lambda x: [df_train_sub['판매수량'][idx] for idx in x])\n",
    "    df_test_sub[f'{k} closest 판매수량'] = df_test_sub[f'{k} closest idx'].apply(lambda x: [df_train_sub['판매수량'][idx] for idx in x])\n",
    "\n",
    "    \n",
    "    return df_train_sub, df_test_sub\n",
    "    \n",
    "def calculate_ad_smape_grouped_by(df_pred,df_true,eval_metric):\n",
    "\n",
    "    if '칼라' in df_pred.columns.tolist():\n",
    "        df_true = df_true[['상품코드','칼라명2','카테고리']]\n",
    "        df_true = df_true.rename(columns={'칼라명2':'칼라'})\n",
    "    \n",
    "        df = df_pred.merge(df_true,how='left',on=['상품코드','칼라'])\n",
    "\n",
    "    else:\n",
    "        df_true = df_true[['상품코드','카테고리']]\n",
    "        df = df_pred.merge(df_true,how='left',on='상품코드')\n",
    "    \n",
    "    cats = df['카테고리'].unique()\n",
    "    dictionary = {}\n",
    "        \n",
    "    for cat in cats:\n",
    "\n",
    "        x = df[df['카테고리'] == cat]\n",
    "\n",
    "        y1 = torch.tensor(x['판매수량_true'].values,dtype=torch.float)\n",
    "        y2 = torch.tensor(x['판매수량_pred'].values,dtype=torch.float)\n",
    "        \n",
    "        eval_score = eval_metric(y1,y2)\n",
    "        dictionary[cat] = eval_score\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class AdjustedSMAPELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "\n",
    "        param epsilon: default 1e-6\n",
    "        \"\"\"\n",
    "        super(AdjustedSMAPELoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "      \n",
    "        numerator = torch.abs(y_pred - y_true)\n",
    "        denominator = torch.abs(y_pred) + torch.abs(y_true) + self.epsilon\n",
    "        \n",
    "        \n",
    "        smape_score = torch.mean(numerator / denominator * 2)/2\n",
    "        \n",
    "        return smape_score\n",
    "\n",
    "class nsr_img_txt_dataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None,fixed_num_images=2):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "        self.fixed_num_images = fixed_num_images\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        image_paths = ast.literal_eval(row['이미지파일'])\n",
    "    \n",
    "        text_embeddings = row.filter(like='설명').values.astype(np.float32)\n",
    "        target = row['normalized_distance']\n",
    "        # target_scaled = row.get('판매수량_scaled',None)\n",
    "        \n",
    "        other_embeddings = row.drop(row.filter(like='설명').index,axis=0)\n",
    "        other_embeddings = other_embeddings.drop(other_embeddings.filter(like='closest').index,axis=0)\n",
    "        other_embeddings = other_embeddings.drop(['판매수량','이미지파일','판매수량_scaled'],errors='ignore').values.astype(np.float32)\n",
    "        \n",
    "    \n",
    "        knn_embeddings = row.filter(like='closest 판매수량').values.item()\n",
    "        \n",
    "        images = [Image.open(image_path) for image_path in image_paths]\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "\n",
    "        while len(images) < self.fixed_num_images:\n",
    "            images.append(images[-1])\n",
    "        images = images[:self.fixed_num_images]\n",
    "        \n",
    "        images_tensor = torch.stack(images)\n",
    "        \n",
    "        text_embeddings_tensor = torch.tensor(text_embeddings,dtype=torch.float32)\n",
    "        other_embeddings_tensor = torch.tensor(other_embeddings,dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target,dtype=torch.float32)\n",
    "        knn_embeddings = torch.tensor(knn_embeddings,dtype=torch.float32)\n",
    "        \n",
    "        if target is not None:\n",
    "\n",
    "   \n",
    "            return images_tensor, text_embeddings_tensor, other_embeddings_tensor, knn_embeddings, target_tensor\n",
    "\n",
    "def datapreprocessing(train_df, test_df, dataloader_generator, image_resize_shape=(256, 256), image_normalizing=False,batch_size=64, shuffle=True, num_workers=None, pin_memory=False):\n",
    "    \"\"\"\n",
    "    train_df : DataFrame for Training\n",
    "    test_df : DataFrame for Testing\n",
    "    image_resize_shape : image_size to reshape \n",
    "    batch_size : Batch size\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from torchvision import transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    assert image_normalizing in [True, False], \"image_normalizing must be either True or False\"\n",
    "    \n",
    "    if image_normalizing == False:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_resize_shape),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_resize_shape),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    \n",
    "    print(transform)\n",
    "\n",
    "    train_dataset = nsr_img_txt_dataset(train_df, transform=transform)\n",
    "    test_dataset = nsr_img_txt_dataset(test_df, transform=transform)\n",
    "\n",
    "    num_workers = num_workers if num_workers is not None else os.cpu_count()\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        generator=dataloader_generator,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size,\n",
    "        generator=dataloader_generator,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "def train(model,optimizer,criterion,eval_metric,train_dataloader, test_dataloader,writer=None, scheduler=None,device='cpu',num_epochs=50,dir_path='model_state_dict',scaler):\n",
    "\n",
    "    os.makedirs(dir_path,exist_ok=True)\n",
    "    model.to(device)\n",
    "\n",
    "    distance_train_eval_list = []\n",
    "    distance_train_loss_list = []\n",
    "    real_value_train_eval_list = []\n",
    "    real_value_train_loss_list = []\n",
    "    \n",
    "    distance_test_eval_list = []\n",
    "    distance_test_loss_list = []\n",
    "    real_value_test_eval_list = []\n",
    "    real_value_test_loss_list = []\n",
    "   \n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Train\n",
    "        model.train()\n",
    "\n",
    "        for param in model.image_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in model.image_model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        distance_train_loss = 0.0\n",
    "        distance_train_eval = 0.0\n",
    "        real_value_train_loss = 0.0\n",
    "        real_value_train_eval = 0.0\n",
    "        \n",
    "        for i, batch in tqdm(enumerate(train_dataloader),desc='Training',leave=False):\n",
    "            images, text_embeddings, other_embeddings, knn_embeddings, targets = [x.to(device) for x in batch]\n",
    "            outputs = model(images, text_embeddings, other_embeddings, knn_embeddings)\n",
    "\n",
    "            distance_loss = criterion(outputs.flatten(),targets.flatten())\n",
    "            distance_eval_score = eval_metric(outputs.flatten(), targets.flatten())\n",
    "\n",
    "            real_value_pred = scaler.inverse_transform(outputs.reshape(-1,1))\n",
    "\n",
    "            real_value_loss = criterion(real_value_pred.flatten(),targets.flatten())\n",
    "            real_value_eval_score = eval_metric(real_value_pred.flatten(), targets.flatten())\n",
    "            \n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"NaN detected in Train outputs at epoch {epoch+1}, batch {i+1}\")\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            distance_train_eval +=  distance_eval_score.item()\n",
    "            distance_train_loss += distance_loss.item()\n",
    "            real_value_train_eval += real_value_eval_score.item()\n",
    "            ral_value_train_loss += real_value_loss.item()\n",
    "\n",
    "        distance_train_eval_list.append(distance_train_eval/len(train_dataloader))\n",
    "        distance_train_loss_list.append(distance_train_loss/len(train_dataloader))\n",
    "        real_value_train_eval_list.append(real_value_train_eval/len(train_dataloader))\n",
    "        real_value_train_loss_list.append(real_value_train_loss/len(train_dataloader))\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        distance_test_loss = 0.0\n",
    "        distance_test_eval = 0.0\n",
    "        real_value_test_loss = 0.0\n",
    "        real_value_test_eval = 0.0\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            for i, batch in tqdm(enumerate(test_dataloader),desc='Testing',leave=False):\n",
    "                images,text_embeddings, other_embeddings,knn_embeddings, targets = [x.to(device) for x in batch]\n",
    "                outputs = model(images, text_embeddings, other_embeddings, knn_embeddings)\n",
    "                \n",
    "                distance_eval_score = eval_metric(outputs.flatten(), targets.flatten())\n",
    "                distance_loss = criterion(outputs.flatten(), targets.flatten())\n",
    "\n",
    "                real_value_pred = scaler.inverse_transform(outputs.reshape(-1,1))\n",
    "\n",
    "                real_value_loss = criterion(real_value_pred.flatten(),targets.flatten())\n",
    "                real_value_eval_score = eval_metric(real_value_pred.flatten(), targets.flatten())\n",
    "            \n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"NaN detected in Test outputs at epoch {epoch+1}, batch {i+1}\")\n",
    "                    continue\n",
    "                \n",
    "                distance_test_eval += distance_eval_score.item()\n",
    "                distance_test_loss += distance_loss.item()\n",
    "                real_value_test_eval += real_value_eval_score.item()\n",
    "                real_value_test_loss += real_value_loss.item()\n",
    "       \n",
    "        distance_test_eval_list.append(distance_test_eval/len(test_dataloader))\n",
    "        distance_test_loss_list.append(distance_test_loss/len(test_dataloader))\n",
    "        real_value_test_eval_list.append(real_value_test_eval/len(test_dataloader))\n",
    "        real_value_test_loss_list.append(real_value_test_loss/len(test_dataloader))\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print(f'\\t For Distance :')\n",
    "        print(f'\\t\\t Loss :')\n",
    "        print(f'\\t\\t\\t Train : {distance_train_loss / len(train_dataloader):.4f}, Test : {distance_test_loss / len(test_dataloader):.4f}' )\n",
    "        print(f'\\t\\t Eval Score :')\n",
    "        print(f'\\t\\t\\t Train : {distance_train_eval / len(train_dataloader):.4f}, Test : {distance_test_eval / len(test_dataloader):.4f}')\n",
    "        \n",
    "        print(f'\\t For Real Value :')\n",
    "        print(f'\\t\\t Loss :')\n",
    "        print(f'\\t\\t\\t Train : {real_value_train_loss / len(train_dataloader):.4f}, Test : {real_value_test_loss / len(test_dataloader):.4f}')\n",
    "        print(f'\\t\\t Eval Score : ')\n",
    "        print(f'\\t\\t\\t Train : {real_value_train_eval / len(train_dataloader):.4f}, Test : {real_value_test_eval / len(test_dataloader):.4f}')\n",
    "\n",
    "        if writer:\n",
    "            # Distance\n",
    "                # Loss\n",
    "            writer.add_scalar(f'Distance/Loss/train',distance_train_loss/len(train_dataloader),epoch+1)\n",
    "            writer.add_scalar(f'Distance/Loss/test',distance_test_loss/len(test_dataloader),epoch+1)\n",
    "                # Eval Score\n",
    "            writer.add_scalar(f'Distance/Eval/train',distance_train_eval/len(train_dataloader),epoch+1)\n",
    "            writer.add_scalar(f'Distance/Eval/test',distance_test_eval/len(train_dataloader),epoch+1)\n",
    "\n",
    "            # Real Value\n",
    "                # Loss\n",
    "            writer.add_scalar(f'Real_Value/Loss/train',real_value_train_loss/len(train_dataloader),epoch+1)\n",
    "            writer.add_scalar(f'Real_Value/Loss/test',real_value_test_loss/len(test_dataloader),epoch+1)\n",
    "                # Eval Score\n",
    "            writer.add_scalar(f'Real_Value/Eval/train',real_value_train_eval/len(train_dataloader),epoch+1)\n",
    "            writer.add_scalar(f'Real_Value/Eval/test',real_value_test_eval/len(train_dataloader),epoch+1)\n",
    "\n",
    "        \n",
    "        if torch.isnan(outputs).any() != True:\n",
    "            if real_value_test_loss/len(test_dataloader) < best_score:\n",
    "                best_score = real_value_test_loss/len(test_dataloader)\n",
    "                torch.save(model.state_dict(), dir_path+'/'+'model.pt')\n",
    "                best_model = model\n",
    "                print(f\"Saved best model at epoch {epoch + 1} with Real Value Target Loss: {best_score:.4f}\")\n",
    "       \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # For Distance \n",
    "        ## Loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(distance_train_loss_list), distance_train_loss_list, label='Train Loss'))\n",
    "        plt.plot(range(len(distance_test_loss_list), distance_test_loss_list, label='Test Loss'))\n",
    "        plt.title('Distance Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(dir_path+'/'+'Distance_Loss_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "        ## Eval Score\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(distance_train_eval_list), distance_train_eval_list, label='Train Eval'))\n",
    "        plt.plot(range(len(distance_test_eval_list), distance_test_eval_list, label='Test Eval'))\n",
    "        plt.title('Distance Eval')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Eval Score')\n",
    "        plt.legend()\n",
    "        plt.savefig(dir_path+'/'+'Distance_Eval_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "        # For Real Value\n",
    "        ## Loss\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(real_value_train_loss_list), real_value_train_loss_list, label='Train Loss'))\n",
    "        plt.plot(range(len(real_value_test_loss_list), real_value_test_loss_list, label='Test Loss'))\n",
    "        plt.title('Real Value Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.savefig(dir_path+'/'+'Real_Value_Loss_plot.png')\n",
    "        plt.close()\n",
    "        ## Eval\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(len(real_value_train_eval_list), real_value_train_eval_list, label='Train Eval'))\n",
    "        plt.plot(range(len(real_value_test_eval_list), real_value_test_eval_list, label='Test Eval'))\n",
    "        plt.title('Real Value Eval')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Eval Score')\n",
    "        plt.legend()\n",
    "        plt.savefig(dir_path+'/'+'Real_Value_Eval_plot.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "\n",
    "       \n",
    "        distance_test_eval_list.append(distance_test_eval/len(test_dataloader))\n",
    "        distance_test_loss_list.append(distance_test_loss/len(test_dataloader))\n",
    "        real_value_test_eval_list.append(real_value_test_eval/len(test_dataloader))\n",
    "        real_value_test_loss_list.append(real_value_test_loss/len(test_dataloader))\n",
    "\n",
    "def test(model,dataset,dataloader,df,df_true,criterion,eval_metric,device,output_dir=str):\n",
    "\n",
    "    model.to(device)\n",
    "    outputs_list = []\n",
    "    distance_test_loss = 0.0\n",
    "    distance_test_eval = 0.0\n",
    "    real_value_test_loss = 0.0\n",
    "    real_value_test_eval = 0.0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for i, batch in tqdm(enumerate(test_dataloader),desc='Testing',leave=False):\n",
    "            images,text_embeddings, other_embeddings,knn_embeddings, targets = [x.to(device) for x in batch]\n",
    "            outputs = model(images, text_embeddings, other_embeddings, knn_embeddings)\n",
    "            \n",
    "            distance_eval_score = eval_metric(outputs.flatten(), targets.flatten())\n",
    "            distance_loss = criterion(outputs.flatten(), targets.flatten())\n",
    "\n",
    "            real_value_pred = scaler.inverse_transform(outputs.reshape(-1,1))\n",
    "\n",
    "            real_value_loss = criterion(real_value_pred.flatten(),targets.flatten())\n",
    "            real_value_eval_score = eval_metric(real_value_pred.flatten(), targets.flatten())\n",
    "        \n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"NaN detected in Test outputs at epoch {epoch+1}, batch {i+1}\")\n",
    "                continue\n",
    "            \n",
    "            distance_test_eval += distance_eval_score.item()\n",
    "            distance_test_loss += distance_loss.item()\n",
    "            real_value_test_eval += real_value_eval_score.item()\n",
    "            real_value_test_loss += real_value_loss.item()\n",
    "            outputs_list.extend(real_value_pred.detach().cpu().numpy())\n",
    "\n",
    "    distance_test_eval /= len(dataloader)\n",
    "    distance_test_loss /= len(dataloader)\n",
    "    real_value_test_eval /= len(dataloader)\n",
    "    real_value_test_loss /= len(dataloader)\n",
    "\n",
    "    outputs_list = [np.round(output.item(),2) for output in outputs_list]\n",
    "   \n",
    "    df['판매수량_pred'] = outputs_list + df['neighbor mean']\n",
    "    saved_path = os.path.join(output_dir,dataset.split('/')[-1].split('.')[0]+'_preds.csv')\n",
    "    \n",
    "    hi = pd.DataFrame()\n",
    "    hi['상품코드'] = df['상품코드']\n",
    "    hi = hi.merge(df.groupby('상품코드')['판매수량_true'].sum(),on='상품코드')\n",
    "    hi = hi.merge(df.groupby('상품코드')['판매수량_pred'].sum(),on='상품코드')\n",
    "    \n",
    "    test_eval_ItemCode = eval_metric(torch.tensor(hi['판매수량_pred'].values),torch.tensor(hi['판매수량_true'].values)).item()\n",
    "    test_loss_ItemCode = criterion(torch.tensor(hi['판매수량_pred'].values),torch.tensor(hi['판매수량_true'].values)).item()\n",
    "    \n",
    "    loss_info_text = dataset.split('/')[-1].split('.')[0] + '_loss_info.txt'\n",
    "    loss_info_path = os.path.join(output_dir, loss_info_text )\n",
    "        \n",
    "    hi.to_csv(os.path.join(output_dir,dataset.split('/')[-1].split('.')[0]+'_상품코드별_preds.csv'),encoding='UTF-8')\n",
    "    df.to_csv(os.path.join(output_dir,dataset.split('/')[-1].split('.')[0]+'_preds.csv'),encoding='UTF-8')\n",
    "    print(f'predict result saved in {saved_path}')\n",
    "\n",
    "    dict_item_loss = calculate_ad_smape_grouped_by(hi,df_true,criterion)\n",
    "    dict_item_eval = calculate_ad_smape_grouped_by(hi,df_true,eval_metric)\n",
    "    \n",
    "    dict_item_color_loss = calculate_ad_smape_grouped_by(df,df_true,criterion)\n",
    "    dict_item_color_eval = calculate_ad_smape_grouped_by(df,df_true,eval_metric)\n",
    "    \n",
    "    with open(loss_info_path, 'w') as f:\n",
    "        f.write(f'Eval Score : {test_loss:.4f}\\n')\n",
    "        f.write(f'Loss Score : {test_eval:.4f}\\n')\n",
    "        f.write(f'상품코드별 Eval Score : {test_eval_ItemCode:.4f}\\n')\n",
    "        f.write(f'상품코드별 Loss Score : {test_loss_ItemCode:.4f}\\n')\n",
    "        for k,v in dict_item_eval.items():\n",
    "            f.write(f'{k} 상품코드별 Eval Score : {v:.4f}\\n')\n",
    "\n",
    "        for k,v in dict_item_loss.items():\n",
    "            f.write(f'{k} 상품코드별 Loss Score : {v:.4f}\\n')\n",
    "            \n",
    "        for k,v in dict_item_color_eval.items():\n",
    "            f.write(f'{k} 상품코드+색상별 Eval Score : {v:.4f}\\n')\n",
    "\n",
    "        for k,v in dict_item_color_loss.items():\n",
    "            f.write(f'{k} 상품코드+색상별 Eval Loss : {v:.4f}\\n')\n",
    "\n",
    "def calculate_adjusted_smape(row, loss_fn):\n",
    "    y_true = torch.tensor([row['판매수량_true']], dtype=torch.float32)\n",
    "    y_pred = torch.tensor([row['판매수량_pred']], dtype=torch.float32)\n",
    "    score = loss_fn(y_pred, y_true).item()\n",
    "    return score\n",
    "\n",
    "    \n",
    "def run(model,dataset,optimizer,criterion,eval_metric,train_dataloader,test_dataloader,dir_path,df,df_true,scheduler=None,writer=None,device='cpu',num_epochs=150):\n",
    "    best_model = train(model,optimizer,criterion,eval_metric,train_dataloader,test_dataloader,writer=writer,scheduler=scheduler,device=device,num_epochs=num_epochs,dir_path=dir_path)\n",
    "    print('Best Model for Train dataset')\n",
    "    test(model=best_model,dataset=dataset[0],dataloader=train_dataloader,df=df[0],df_true=df_true[0],criterion=criterion,eval_metric=eval_metric,device=device,output_dir=dir_path)\n",
    "    print('Best Model for Test dataset')\n",
    "    test(model=best_model,dataset=dataset[1],dataloader=test_dataloader,df=df[1],df_true=df_true[1],criterion=criterion,eval_metric=eval_metric,device=device,output_dir=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7359be6f-b0fc-4b57-be16-8feb48ecd725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# set random seed\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(args.random_seed)\n",
    "\n",
    "train_dataloader, test_dataloader = datapreprocessing(df_train, df_test, generator, image_resize_shape=(args.image_size, args.image_size), image_normalizing=args.image_normalizing,batch_size=args.batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
    "\n",
    "text_embedding_dim_in = len(df_train.filter(like='설명').columns)\n",
    "other_features_dim_in = df_train.drop(df_train.filter(like='설명').columns,axis=1).shape[-1]-3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23735aec-0576-4058-886c-eb77318d16fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sungheon",
   "language": "python",
   "name": "sungheon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
