{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from engines.week16_engines4_mse_knn_trial import *\n",
    "# from engines.models_transformer import *\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox,boxcox1p,inv_boxcox1p\n",
    "\n",
    "\n",
    "\n",
    "def none_or_int(value):\n",
    "    if value.lower() == 'none':\n",
    "        return None\n",
    "    return int(value)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Training script for MultiModalMOdel')\n",
    "\n",
    "parser.add_argument('--device',type=int,default=0,help='device number for gpu accelerating (default = 0)')\n",
    "parser.add_argument('--batch_size',type=int,default=68,help='batch size (default=64)')\n",
    "parser.add_argument('--image_embeddings_dim_out', type=none_or_int, default=128, help='Add Linear Layer, Output dimension for image embeddings (default=128, set \"None\" for no linear embedding proj)')\n",
    "parser.add_argument('--text_embeddings_dim_out', type=none_or_int, default=128, help='Add Linear Layer, Output dimension for text embeddings (default=128, set \"None\" for no linear embedding proj))')\n",
    "parser.add_argument('--other_features_dim_out', type=none_or_int, default=None, help='Add Linear Layer, Output dimension for other features embeddings (default=128, set \"None\" for no linear embedding proj))')\n",
    "parser.add_argument('--header_mode',type=str,default='FFN',help='FFN, Dense, Transformer, FFN_Transformer, Dense_Transformer')\n",
    "parser.add_argument('--header_hidden_dims', type=int, nargs='+', default=[128], help='Add Linear Layer, Hidden dimensions for the header (default=128)')\n",
    "parser.add_argument('--dir_path', type=str, default='this_is_experiment', help='Directory path to save the model state dict (default=\"model_state_dict\")') \n",
    "parser.add_argument('--train_dataset',type=str, default='/home/sflab/SFLAB/sungheon/nsr/dataset/nsr_train_할인율0_중분류.csv',help='train dataset path (default=\"../dataset/nsr_train.csv\")')\n",
    "parser.add_argument('--test_dataset',type=str, default='/home/sflab/SFLAB/sungheon/nsr/dataset/nsr_test_할인율0_중분류.csv',help='test dataset path (default=\"../dataset/nsr_test.csv\")')\n",
    "parser.add_argument('--random_seed',type=int, default=42,help='random seed (default=42)')\n",
    "parser.add_argument('--image_size',type=int,default=256,help='image size for transforming (default=256)')\n",
    "parser.add_argument('--learning_rate',type=float,default=0.01,help='learning rate (default=0.0000001)')\n",
    "parser.add_argument('--num_epochs',type=int,default=50,help='Num epochs for training (default=50)')\n",
    "parser.add_argument('--loss_ratio',type=float,default=1,help='Scaled/Unscaled Loss ratio, default=1(totally weight on target Scaled loss (default=1)')\n",
    "parser.add_argument('--image_normalizing',type=bool,default=False,help='Decide wheter use image nomralizing or not(default=False)')\n",
    "parser.add_argument('--nhead',type=int,default=4,help='transformer nhead')\n",
    "parser.add_argument('--num_encoder_layers',type=int,default=6,help='transformer num encoder layers')\n",
    "parser.add_argument('--num_decoder_layers',type=int,default=6,help='transformer num decoder layers')\n",
    "parser.add_argument('--model_path',type=str,default='engines/models_transformer_knn_trial.py',help='model.py path')\n",
    "parser.add_argument('--activation_func',type=str,default='gelu',help='gelu or relu')\n",
    "\n",
    "if '--help' in sys.argv or '-h' in sys.argv:\n",
    "    parser.print_help()\n",
    "    sys.exit()\n",
    "    \n",
    "if 'ipykernel' in sys.modules:\n",
    "    # Jupyter 노트북에서 실행 중인 경우\n",
    "    args = parser.parse_args(args=[])\n",
    "else:\n",
    "    # 일반 스크립트로 실행 중인 경우\n",
    "    args = parser.parse_args()\n",
    "\n",
    "model_path = args.model_path\n",
    "model_py = model_path.split('/')[-1]\n",
    "if model_py == 'models_transformer.py':\n",
    "    from engines.models_transformer import *\n",
    "elif model_py == 'models_transformer_v2.py':\n",
    "    from engines.models_transformer_v2 import *\n",
    "elif model_py == 'models_transformer_knn_trial.py':\n",
    "    from engines.models_transformer_knn_trial import *\n",
    "\n",
    "set_random_seed(args.random_seed)\n",
    "\n",
    "os.makedirs(args.dir_path, exist_ok=True)\n",
    "config_path = os.path.join(args.dir_path, 'config.json')\n",
    "with open(config_path, 'w',encoding='utf-8') as f:\n",
    "    json.dump(vars(args), f, indent=4,ensure_ascii=False)\n",
    "\n",
    "df_train = pd.read_csv(args.train_dataset)\n",
    "df_train = df_train.drop(columns=['Unnamed: 0', '판매시작연도', '판매첫날', '상품코드', '판매일자', '상품명', '상품명2', '칼라', '칼라명', '칼라명2', '현재가', '할인율(%)', '파일경로', '이미지갯수', '외관설명', '기능설명','카테고리'],errors='ignore')\n",
    "\n",
    "cols = df_train.columns.tolist()\n",
    "df_test = pd.read_csv(args.test_dataset)\n",
    "df_test = df_test[cols]\n",
    "df_train = df_train.drop_duplicates()\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "y_train = np.array(df_train['판매수량'].tolist())\n",
    "y_test = np.array(df_test['판매수량'].tolist())\n",
    "\n",
    "# y_train = y_train +1\n",
    "# y_test = y_test + 1\n",
    "\n",
    "boxcox_y_train, lambda_train = boxcox(y_train)\n",
    "\n",
    "lowerbound = np.quantile(y_train, 0.16)\n",
    "upperbound = np.quantile(y_train, 0.84)\n",
    "\n",
    "a = 2 / (lowerbound**lambda_train - upperbound**lambda_train)\n",
    "b = a * (lowerbound**lambda_train) - 1\n",
    "\n",
    "y_train_boxcoxed_with_shifting = a * (y_train**lambda_train) - b\n",
    "y_test_boxcoxed_with_shifting = a * (y_test**lambda_train) - b\n",
    "# df_train['판매수량_scaled'] = boxcox_y_train\n",
    "# df_test['판매수량_scaled'] = boxcox_y_test\n",
    "df_train['판매수량_scaled'] = y_train_boxcoxed_with_shifting\n",
    "df_test['판매수량_scaled'] = y_test_boxcoxed_with_shifting\n",
    "\n",
    "# set random seed\n",
    "generator = torch.Generator()\n",
    "generator.manual_seed(args.random_seed)\n",
    "\n",
    "train_dataloader, test_dataloader = datapreprocessing(df_train, df_test, generator, image_resize_shape=(args.image_size, args.image_size), image_normalizing=args.image_normalizing,batch_size=args.batch_size, num_workers=os.cpu_count(), pin_memory=True)\n",
    "\n",
    "# define embdding dims\n",
    "text_embedding_dim_in = len(df_train.filter(like='설명').columns)\n",
    "other_features_dim_in = df_train.drop(df_train.filter(like='설명').columns,axis=1).shape[-1]-3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 108 entries, 0 to 4898\n",
      "Columns: 1557 entries, 이미지파일 to 판매수량_scaled\n",
      "dtypes: bool(15), float64(1537), int64(4), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/sflab/SFLAB/sungheon/nsr/public/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from engines.week16_engines4_mse_knn_trial import *\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox,boxcox1p,inv_boxcox1p\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "class MULTIIMBEDDING(nn.Module):\n",
    "    def __init__(self, num_images_per_data, header_mode:str='ffn',\n",
    "                 image_embeddings_dim_out=None, text_embedding_dim_in=None, \n",
    "                 text_embedding_dim_out=None, other_features_dim_in=None, \n",
    "                 other_features_dim_out=None, header_hidden_dims:list=None,\n",
    "                 nhead:int=8, num_encoder_layers:int=6,num_decoder_layers:int=6,activation:str='gelu'):\n",
    "        \n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        header_mode : {'ffn', 'dense', 'transformer','ffn_transformer','dense_transformer'} 이 중에서 선택해주세요.\n",
    "        num_images_per_data : 데이터 포인트 당 이미지 갯수 (데이터 로드 하는 과정에서 1개 가진 이미지도 복제 하여 모두 2개로 만들어 놨습니다. 수정하실 필요 없이 2로 고정하시면 됩니다.)\n",
    "        image_embeddings_dim_out : 원하는 이미지 emedding dimension (맘대로 가능)\n",
    "        text_embedding_dim_in : dataframe의 텍스트 칼럼 dimension (데이터 프레임에 있는 텍스트 임베딩 칼럼 갯수.. 고정값)\n",
    "        text_embedding_dim_out : 선형 변환을 통한 텍스트 embedding dim (맘대로 가능)\n",
    "        other_features_dim_in : dataframe의 텍스트 embedding 칼럼 및 이미지, target 칼럼 제외, 나머지 feature 칼럼들 dimension (데이터 프레임에 있는 텍스트 제외한 칼럼들 갯수.. 고정값)\n",
    "        other_features_dim_out : 선형 변환을 통한 output feature dim (맘대로 가능)\n",
    "        header_hidden_dims : 각 모델의 ffn이나, dense 블록의 hidden dim\n",
    "        nhead : transformer 계열 모델에서 num head,\n",
    "        num_encoder_layers : transformer의 encoder layer 갯수\n",
    "        num_decoder_layers : transformer의 decoder layer 갯수\n",
    "        \"\"\"\n",
    "        \n",
    "        # Image\n",
    "        self.header_mode = header_mode\n",
    "        self.num_images = num_images_per_data\n",
    "        self.image_model_weights = torchvision.models.ResNet152_Weights.DEFAULT\n",
    "        self.image_model = torchvision.models.resnet152(weights=self.image_model_weights)\n",
    "        \n",
    "        if image_embeddings_dim_out is not None:\n",
    "            self.image_model.fc = nn.Linear(in_features=self.image_model.fc.in_features, out_features=image_embeddings_dim_out)\n",
    "            self.image_output_dim = image_embeddings_dim_out\n",
    "        else:\n",
    "            self.image_output_dim = self.image_model.fc.in_features\n",
    "            self.image_model.fc = nn.Identity()\n",
    "\n",
    "        # Text\n",
    "        if text_embedding_dim_out is not None:\n",
    "            self.text_fc = nn.Linear(in_features=text_embedding_dim_in, out_features=text_embedding_dim_out)\n",
    "            self.text_embedding_dim = text_embedding_dim_out\n",
    "        else:\n",
    "            self.text_fc = nn.Identity()\n",
    "            self.text_embedding_dim = text_embedding_dim_in\n",
    "\n",
    "        # 나머지 feature\n",
    "        if other_features_dim_out is not None:\n",
    "            self.rest_feature_fc = nn.Linear(in_features=other_features_dim_in, out_features=other_features_dim_out)\n",
    "            self.other_features_dim = other_features_dim_out\n",
    "        else:\n",
    "            self.rest_feature_fc = nn.Identity()\n",
    "            self.other_features_dim = other_features_dim_in\n",
    "\n",
    "        print('image_output_dim :',self.image_output_dim)\n",
    "        print('text_embedding_dim :',self.text_embedding_dim)\n",
    "        print('other_features_dim :',self.other_features_dim)\n",
    "        # 막단 layer\n",
    "        self.input_dim = self.image_output_dim * self.num_images + self.text_embedding_dim + self.other_features_dim\n",
    "        if activation.lower() == 'relu':\n",
    "            self.activ = nn.ReLU()\n",
    "        elif activation.lower() == 'gelu':\n",
    "            self.activ = nn.GELU()\n",
    "                \n",
    "    def forward(self, images, text, other_features):\n",
    "        image_embeddings = [self.activ(self.image_model(image)) for image in images]\n",
    "        image_embeddings = torch.flatten(torch.stack(image_embeddings, dim=0), start_dim=1)\n",
    "\n",
    "        text_embeddings = self.text_fc(text)\n",
    "        text_embeddings = self.activ(text_embeddings)\n",
    "        \n",
    "        other_embeddings = self.rest_feature_fc(other_features)\n",
    "        other_embeddings = self.activ(other_embeddings)\n",
    "        \n",
    "        combined_embeddings = torch.cat((image_embeddings, text_embeddings, other_embeddings), dim=1)\n",
    "        return combined_embeddings\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_output_dim : 128\n",
      "text_embedding_dim : 128\n",
      "other_features_dim : 18\n"
     ]
    }
   ],
   "source": [
    "model = MULTIIMBEDDING(num_images_per_data=2,\n",
    "                        header_mode= args.header_mode,\n",
    "                        image_embeddings_dim_out=args.image_embeddings_dim_out,\n",
    "                        text_embedding_dim_in=text_embedding_dim_in,\n",
    "                        text_embedding_dim_out=args.text_embeddings_dim_out,\n",
    "                        other_features_dim_in=other_features_dim_in,\n",
    "                        other_features_dim_out=args.other_features_dim_out,\n",
    "                        header_hidden_dims=args.header_hidden_dims,\n",
    "                        nhead = args.nhead,\n",
    "                        num_encoder_layers = args.num_encoder_layers,\n",
    "                        num_decoder_layers = args.num_decoder_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(model,train_dataloader, test_dataloader,device='cpu',dir_path='model_state_dict',a=None,b=None,boxcox_lambda=None):\n",
    "\n",
    "    os.makedirs(dir_path,exist_ok=True)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    for param in model.image_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.image_model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "        for i, batch in tqdm(enumerate(train_dataloader),desc='Training',leave=False):\n",
    "            images, text_embeddings, other_embeddings, targets_unscaled, targets_scaled = [x.to(device) for x in batch]\n",
    "            output_embedding = model(images, text_embeddings, other_embeddings)\n",
    "            return output_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                            \r"
     ]
    }
   ],
   "source": [
    "a = embed(model,train_dataloader,test_dataloader,device='cuda:0',dir_path=args.dir_path,a=a,b=b,boxcox_lambda=lambda_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([107, 402])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dist = euclidean_distances(a.cpu().detach().numpy(), a.cpu().detach().numpy())\n",
    "np.fill_diagonal(train_dist, np.Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      inf, 1.9505497, 1.4213275, ..., 0.893373 , 1.8796357,\n",
       "        1.8881046],\n",
       "       [1.9505497,       inf, 1.4633942, ..., 1.93993  , 1.5065978,\n",
       "        1.600001 ],\n",
       "       [1.4213275, 1.4633942,       inf, ..., 1.4698195, 1.8692372,\n",
       "        1.4249022],\n",
       "       ...,\n",
       "       [0.893373 , 1.93993  , 1.4698195, ...,       inf, 1.9120259,\n",
       "        1.9614851],\n",
       "       [1.8796357, 1.5065978, 1.8692372, ..., 1.9120259,       inf,\n",
       "        1.8848342],\n",
       "       [1.8881046, 1.600001 , 1.4249022, ..., 1.9614851, 1.8848342,\n",
       "              inf]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hs_f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
